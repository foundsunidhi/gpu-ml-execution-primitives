# GPU ML Execution Primitives Research

## Objective
Build a deep understanding of how GPUs execute machine learning workloads,
with a focus on translating GPU efficiency to CPU-scale inference.

## Scope
- GPU architecture taxonomy (NVIDIA, AMD)
- Tensor / Matrix cores across generations
- GEMM and attention execution analysis
- Memory hierarchy and latency hiding
- Kernel optimization techniques
- Workload bottleneck classification
- CPU implications for ML inference

## Motivation
Inspired by recent advances such as Google Gemma’s application in scientific
discovery, where inference efficiency is critical for scaling research.

## Repository Structure
See `/data` for architecture specs, execution traces, and optimization catalogs.

## Status
Week 1 – GPU architecture & matrix unit analysis (in progress)
