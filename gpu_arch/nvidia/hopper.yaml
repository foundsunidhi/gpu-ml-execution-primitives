schema_version: 1.0

architecture:
  vendor: NVIDIA
  name: Hopper
  compute_capability: 9.0
  launch_year: 2022
  primary_products:
    - H100

objective: >
  Document the NVIDIA Hopper GPU microarchitecture, focusing on
  Transformer-centric execution, FP8 tensor computation,
  and advanced asynchronous memory movement mechanisms.

# Compute Hierarchy

compute_hierarchy:
  processing_unit:
    name: Streaming Multiprocessor (SM)
    count_per_gpu: 120                  # H100 SXM
    cores_per_sm:
      fp32_cores: 128
      fp64_cores: 64
      int32_cores: 64

  thread_group:
    name: Warp
    width: 32
    scheduling:
      warp_schedulers_per_sm: 4
      dispatch_units_per_sm: 8
      issue_model: Independent thread scheduling
      thread_block_clusters: true

# Matrix / Tensor Acceleration Units

matrix_units:
  name: Tensor Cores
  generation: Fourth-generation Tensor Cores
  units_per_sm: 4
  operations_supported:
    - D = A Ã— B + C
    - Transformer Engine mixed-precision MMA

  supported_precisions:
    dense:
      - FP64
      - FP32
      - TF32
      - FP16
      - BF16
      - FP8 (E4M3)
      - FP8 (E5M2)
      - INT8
    sparse:
      - FP16
      - BF16
      - TF32
      - FP8

  peak_tensor_throughput:
    dense:
      fp8: 1000+ TFLOPS
      fp16: 1000+ TFLOPS
      tf32: 500+ TFLOPS
    sparse:
      fp8: 2000+ TFLOPS

  programming_interfaces:
    - CUDA WMMA API
    - CUDA MMA API
    - PTX MMA instructions
    - Transformer Engine (automatic precision selection)

# Memory Hierarchy

memory_hierarchy:
  registers:
    register_file_size_per_sm: 256 KB
    max_registers_per_thread: 255

  shared_memory:
    size_per_sm: 228 KB
    unified_l1_shared: true
    bank_width: 4 bytes

  l1_cache:
    unified_with_shared: true
    cache_line_size: 128 bytes

  l2_cache:
    total_size: 60 MB
    cache_line_size: 64 bytes

  device_memory:
    type: HBM3
    capacity: 80 GB
    bandwidth: 3000+ GB/s

# Instruction Set Architecture Features

isa_features:
  matrix_math:
    - FP8 MMA instructions
    - Transformer Engine dynamic precision MMA
    - Sparse MMA

  memory:
    - Tensor Memory Accelerator (TMA)
    - Asynchronous bulk tensor transfers
    - cp.async (enhanced)

  synchronization:
    - Cooperative Groups
    - Warp-level primitives
    - Thread block cluster synchronization
    - Asynchronous transaction barriers (mbarrier)

# Performance Characteristics

performance_characteristics:
  compute_throughput:
    fp32_peak: 60 TFLOPS
    fp64_peak: 30 TFLOPS

  memory_system:
    l2_bandwidth: Designed to absorb attention KV-cache traffic
    hbm_bandwidth: Sustains large sequence-length workloads

  occupancy_factors:
    max_threads_per_sm: 2048
    max_warps_per_sm: 64
    max_blocks_per_sm: 32

# Architectural Innovations (Relative to Ampere)

architectural_innovations:
  - FP8 data types (E4M3, E5M2) for transformer workloads
  - Transformer Engine for automatic precision management
  - Tensor Memory Accelerator (TMA)
  - Thread Block Clusters for multi-SM cooperation
  - Significantly expanded shared memory and L2 cache

# Source Attribution

sources:
  official_documents:
    - NVIDIA Hopper Architecture Whitepaper
    - NVIDIA H100 GPU Datasheet
    - CUDA C Programming Guide (Hopper section)
    - PTX ISA Documentation (SM90)
  tools_and_measurement:
    - Nsight Compute Metrics Guide
  academic_references:
    - NVIDIA H100 Transformer Engine Technical Overview
